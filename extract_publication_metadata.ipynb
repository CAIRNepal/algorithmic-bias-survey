{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DOI list\n",
    "dois = [\n",
    "    \"doi.org/10.1038/s41591-019-0726-6\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf4ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data extraction...\n",
      "Cleaned DOIs: ['10.1038/s41591-019-0726-6']...\n",
      "Fetching -- https://api.openalex.org/works?filter=doi:10.1038/s41591-019-0726-6&per-page=50\n",
      "Response: 200\n",
      "Found 1 publications in this chunk\n",
      "Total publications found: 1\n",
      "Processing 1 publications...\n",
      "Data saved to publication_data_enhanced_(93-94).csv\n",
      "                                               title  \\\n",
      "0  Diagnosing bias in data-driven algorithms for ...   \n",
      "\n",
      "                                         all_authors  \\\n",
      "0  Jenna Wiens; W. Nicholson Price; Michael W. Sj...   \n",
      "\n",
      "                                    all_affiliations  all_countries  \n",
      "0  University of Michigan; University of Michigan...  USA; USA; USA  \n",
      "Final dataset: 1 publications (including any N/A rows)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "# Split a list into chunks of specified size\n",
    "def chunk_list(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "# Clean DOI strings by removing common prefixes\n",
    "def clean_doi(doi):\n",
    "    prefixes_to_remove = [\n",
    "        \"https://doi.org/\",\n",
    "        \"http://doi.org/\",\n",
    "        \"doi.org/\",\n",
    "        \"DOI:\",\n",
    "        \"doi:\",\n",
    "        \"https://dx.doi.org/\",\n",
    "        \"http://dx.doi.org/\",\n",
    "    ]\n",
    "\n",
    "    cleaned_doi = doi.strip()\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_doi.startswith(prefix):\n",
    "            cleaned_doi = cleaned_doi[len(prefix) :]\n",
    "            break\n",
    "\n",
    "    return cleaned_doi\n",
    "\n",
    "\n",
    "# Fetch publication metadata from OpenAlex API with retry mechanism\n",
    "def get_publication_data(dois, retries=3, delay=5):\n",
    "    all_results = []\n",
    "    cleaned_dois = [clean_doi(doi) for doi in dois]\n",
    "    print(f\"Cleaned DOIs: {cleaned_dois[:5]}...\")\n",
    "\n",
    "    for chunk in chunk_list(cleaned_dois, 50):\n",
    "        pipe_separated_dois = \"|\".join(chunk)\n",
    "        url = f\"https://api.openalex.org/works?filter=doi:{pipe_separated_dois}&per-page=50\"\n",
    "        print(f\"Fetching -- {url}\")\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                print(f\"Response: {response.status_code}\")\n",
    "                if response.status_code == 200:\n",
    "                    results = response.json().get(\"results\", [])\n",
    "                    print(f\"Found {len(results)} publications in this chunk\")\n",
    "                    all_results.extend(results)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Error {response.status_code} fetching DOIs, retrying... ({attempt+1}/{retries})\"\n",
    "                    )\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request error: {e}, retrying... ({attempt+1}/{retries})\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(f\"Total publications found: {len(all_results)}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Extract grant information from publication data safely\n",
    "def extract_grants(publication):\n",
    "    grants_extracted = []\n",
    "    grants_data = publication.get(\"grants\", [])\n",
    "\n",
    "    if not isinstance(grants_data, list):\n",
    "        return grants_extracted\n",
    "\n",
    "    for grant in grants_data:\n",
    "        if not isinstance(grant, dict):\n",
    "            continue\n",
    "\n",
    "        funder_info = grant.get(\"funder\", {})\n",
    "        if not isinstance(funder_info, dict):\n",
    "            continue\n",
    "\n",
    "        grants_extracted.append(\n",
    "            {\n",
    "                \"funder\": funder_info.get(\"display_name\", None),\n",
    "                \"funder_id\": funder_info.get(\"id\", None),\n",
    "                \"award_id\": grant.get(\"award_id\", None),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return grants_extracted\n",
    "\n",
    "\n",
    "# Extract keywords as a list of display names\n",
    "def extract_keywords(publication):\n",
    "    kw_data = publication.get(\"keywords\", [])\n",
    "    if not isinstance(kw_data, list):\n",
    "        return []\n",
    "\n",
    "    keywords_list = []\n",
    "    for kw in kw_data:\n",
    "        if isinstance(kw, dict):\n",
    "            display_name = kw.get(\"display_name\")\n",
    "            if display_name:\n",
    "                keywords_list.append(display_name)\n",
    "\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "# Country code to country name mapping\n",
    "COUNTRY_CODE_TO_NAME = {\n",
    "    \"US\": \"USA\",\n",
    "    \"GB\": \"United Kingdom\",\n",
    "    \"CA\": \"Canada\",\n",
    "    # ... (other mappings)\n",
    "    \"NP\": \"Nepal\",\n",
    "}\n",
    "\n",
    "\n",
    "# Extract relevant metadata from publication list\n",
    "def extract_publication_data(publication_data):\n",
    "    publication_rows = []\n",
    "\n",
    "    for publication in publication_data:\n",
    "        authorships = publication.get(\"authorships\", [])\n",
    "        if not isinstance(authorships, list):\n",
    "            continue\n",
    "\n",
    "        keywords_list = extract_keywords(publication)\n",
    "\n",
    "        all_authors = []\n",
    "        all_affiliations = []\n",
    "        all_countries = []\n",
    "\n",
    "        for author in authorships:\n",
    "            author_name = author.get(\"author\", {}).get(\"display_name\", None)\n",
    "            if author_name:\n",
    "                all_authors.append(author_name)\n",
    "\n",
    "            author_affiliation = \"\"\n",
    "            author_country = \"\"\n",
    "\n",
    "            institution_info = author.get(\"institutions\")\n",
    "            if (\n",
    "                institution_info\n",
    "                and isinstance(institution_info, list)\n",
    "                and len(institution_info) > 0\n",
    "            ):\n",
    "                primary_inst = institution_info[0]\n",
    "                if isinstance(primary_inst, dict):\n",
    "                    aff = primary_inst.get(\"display_name\")\n",
    "                    if aff:\n",
    "                        author_affiliation = aff\n",
    "\n",
    "                    ctry_code = primary_inst.get(\"country_code\")\n",
    "                    if ctry_code:\n",
    "                        ctry_name = COUNTRY_CODE_TO_NAME.get(ctry_code, ctry_code)\n",
    "                        author_country = ctry_name\n",
    "\n",
    "            all_affiliations.append(author_affiliation)\n",
    "            all_countries.append(author_country)\n",
    "\n",
    "        publication_row = {\n",
    "            \"id\": publication.get(\"id\", None),\n",
    "            \"title\": publication.get(\"title\", None),\n",
    "            \"display_name\": publication.get(\"display_name\", None),\n",
    "            \"all_authors\": \"; \".join(all_authors) if all_authors else \"\",\n",
    "            \"all_affiliations\": \"; \".join(all_affiliations) if all_affiliations else \"\",\n",
    "            \"all_countries\": \"; \".join(all_countries) if all_countries else \"\",\n",
    "            \"doi\": publication.get(\"doi\", None),\n",
    "            \"publication_date\": publication.get(\"publication_date\", None),\n",
    "            \"publication_year\": publication.get(\"publication_year\", None),\n",
    "            \"type\": publication.get(\"type\", None),\n",
    "            \"language\": publication.get(\"language\", None),\n",
    "            \"open_access\": publication.get(\"open_access\", {}).get(\"is_oa\", None),\n",
    "            \"open_access_status\": publication.get(\"open_access\", {}).get(\n",
    "                \"oa_status\", None\n",
    "            ),\n",
    "            \"open_access_url\": publication.get(\"open_access\", {}).get(\"oa_url\", None),\n",
    "            \"cited_by_count\": publication.get(\"cited_by_count\", None),\n",
    "            \"keywords\": \"; \".join(keywords_list) if keywords_list else \"\",\n",
    "            \"grants\": (\n",
    "                \"; \".join(\n",
    "                    [\n",
    "                        grant.get(\"funder\", \"\")\n",
    "                        for grant in extract_grants(publication)\n",
    "                        if grant.get(\"funder\")\n",
    "                    ]\n",
    "                )\n",
    "                if extract_grants(publication)\n",
    "                else \"\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        publication_rows.append(publication_row)\n",
    "\n",
    "    return publication_rows\n",
    "\n",
    "\n",
    "# Order DataFrame according to original DOI list and handle missing entries\n",
    "def order_by_doi_sequence(df, original_dois):\n",
    "    cleaned_original_dois = [clean_doi(doi) for doi in original_dois]\n",
    "\n",
    "    if df.empty:\n",
    "        na_rows = [\n",
    "            {\n",
    "                \"id\": \"N/A\",\n",
    "                \"title\": \"N/A\",\n",
    "                \"display_name\": \"N/A\",\n",
    "                \"all_authors\": \"N/A\",\n",
    "                \"all_affiliations\": \"N/A\",\n",
    "                \"all_countries\": \"N/A\",\n",
    "                \"doi\": f\"https://doi.org/{doi}\",\n",
    "                \"publication_date\": \"N/A\",\n",
    "                \"publication_year\": \"N/A\",\n",
    "                \"type\": \"N/A\",\n",
    "                \"language\": \"N/A\",\n",
    "                \"open_access\": \"N/A\",\n",
    "                \"open_access_status\": \"N/A\",\n",
    "                \"open_access_url\": \"N/A\",\n",
    "                \"cited_by_count\": \"N/A\",\n",
    "                \"keywords\": \"N/A\",\n",
    "                \"grants\": \"N/A\",\n",
    "            }\n",
    "            for doi in cleaned_original_dois\n",
    "        ]\n",
    "        return pd.DataFrame(na_rows)\n",
    "\n",
    "    def extract_doi_from_url(doi_url):\n",
    "        if pd.isna(doi_url) or not doi_url:\n",
    "            return \"\"\n",
    "        return (\n",
    "            doi_url.replace(\"https://doi.org/\", \"\")\n",
    "            if doi_url.startswith(\"https://doi.org/\")\n",
    "            else doi_url\n",
    "        )\n",
    "\n",
    "    found_dois = set(extract_doi_from_url(doi) for doi in df[\"doi\"])\n",
    "\n",
    "    missing_dois = [doi for doi in cleaned_original_dois if doi not in found_dois]\n",
    "\n",
    "    na_rows = [\n",
    "        {\n",
    "            \"id\": \"N/A\",\n",
    "            \"title\": \"N/A\",\n",
    "            \"display_name\": \"N/A\",\n",
    "            \"all_authors\": \"N/A\",\n",
    "            \"all_affiliations\": \"N/A\",\n",
    "            \"all_countries\": \"N/A\",\n",
    "            \"doi\": f\"https://doi.org/{doi}\",\n",
    "            \"publication_date\": \"N/A\",\n",
    "            \"publication_year\": \"N/A\",\n",
    "            \"type\": \"N/A\",\n",
    "            \"language\": \"N/A\",\n",
    "            \"open_access\": \"N/A\",\n",
    "            \"open_access_status\": \"N/A\",\n",
    "            \"open_access_url\": \"N/A\",\n",
    "            \"cited_by_count\": \"N/A\",\n",
    "            \"keywords\": \"N/A\",\n",
    "            \"grants\": \"N/A\",\n",
    "        }\n",
    "        for doi in missing_dois\n",
    "    ]\n",
    "\n",
    "    df_with_na = (\n",
    "        pd.concat([df, pd.DataFrame(na_rows)], ignore_index=True) if na_rows else df\n",
    "    )\n",
    "\n",
    "    doi_to_order = {doi: i for i, doi in enumerate(cleaned_original_dois)}\n",
    "\n",
    "    df_with_na[\"doi_order\"] = df_with_na[\"doi\"].apply(\n",
    "        lambda x: doi_to_order.get(extract_doi_from_url(x), 999999)\n",
    "    )\n",
    "\n",
    "    df_ordered = (\n",
    "        df_with_na.sort_values(\"doi_order\")\n",
    "        .drop(columns=[\"doi_order\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_ordered\n",
    "\n",
    "\n",
    "# Fetch publication data\n",
    "print(\"Starting data extraction...\")\n",
    "data = get_publication_data(dois)\n",
    "print(f\"Processing {len(data)} publications...\")\n",
    "\n",
    "if not data:\n",
    "    print(\"No data extracted. Adding N/A rows...\")\n",
    "    df_final = order_by_doi_sequence(pd.DataFrame(), dois)\n",
    "else:\n",
    "    records = extract_publication_data(data)\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    seen_ids = set()\n",
    "    indices_to_keep = [\n",
    "        i\n",
    "        for i, pub_id in enumerate(df[\"id\"])\n",
    "        if not (pub_id in seen_ids or seen_ids.add(pub_id))\n",
    "    ]\n",
    "\n",
    "    df_deduplicated = df.iloc[indices_to_keep].reset_index(drop=True)\n",
    "\n",
    "    df_final = order_by_doi_sequence(df_deduplicated, dois)\n",
    "\n",
    "# Save results to CSV\n",
    "df_final.to_csv(\"publication_data_enhanced_(1).csv\", index=False)\n",
    "print(\"Data saved to publication_data_enhanced_(93-94).csv\")\n",
    "print(df_final[[\"title\", \"all_authors\", \"all_affiliations\", \"all_countries\"]].head(3))\n",
    "print(f\"Final dataset: {len(df_final)} publications (including any N/A rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c409d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>display_name</th>\n",
       "      <th>all_authors</th>\n",
       "      <th>all_affiliations</th>\n",
       "      <th>all_countries</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>open_access</th>\n",
       "      <th>open_access_status</th>\n",
       "      <th>open_access_url</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>keywords</th>\n",
       "      <th>grants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W4388757737</td>\n",
       "      <td>Intentional Biases in LLM Responses</td>\n",
       "      <td>Intentional Biases in LLM Responses</td>\n",
       "      <td>Nicklaus Badyal; Derek Jacoby; Yvonne Coady</td>\n",
       "      <td>University of Victoria; University of Victoria...</td>\n",
       "      <td>Canada; Canada; Canada</td>\n",
       "      <td>https://doi.org/10.1109/uemcon59035.2023.10316060</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>article</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>green</td>\n",
       "      <td>https://arxiv.org/pdf/2311.07611</td>\n",
       "      <td>4</td>\n",
       "      <td>Viewpoints; Persona; Supervisor</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                                title  \\\n",
       "0  https://openalex.org/W4388757737  Intentional Biases in LLM Responses   \n",
       "\n",
       "                          display_name  \\\n",
       "0  Intentional Biases in LLM Responses   \n",
       "\n",
       "                                   all_authors  \\\n",
       "0  Nicklaus Badyal; Derek Jacoby; Yvonne Coady   \n",
       "\n",
       "                                    all_affiliations           all_countries  \\\n",
       "0  University of Victoria; University of Victoria...  Canada; Canada; Canada   \n",
       "\n",
       "                                                 doi publication_date  \\\n",
       "0  https://doi.org/10.1109/uemcon59035.2023.10316060       2023-10-12   \n",
       "\n",
       "   publication_year     type language  open_access open_access_status  \\\n",
       "0              2023  article       en         True              green   \n",
       "\n",
       "                    open_access_url  cited_by_count  \\\n",
       "0  https://arxiv.org/pdf/2311.07611               4   \n",
       "\n",
       "                          keywords grants  \n",
       "0  Viewpoints; Persona; Supervisor         "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACT WITH ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data extraction...\n",
      "Cleaned DOIs: ['10.1038/s41591-019-0726-6']...\n",
      "Fetching -- https://api.openalex.org/works?filter=doi:10.1038/s41591-019-0726-6&per-page=50\n",
      "Response: 200\n",
      "Found 1 publications in this chunk\n",
      "Total publications found: 1\n",
      "Processing 1 publications...\n",
      "Data saved to publication_data_enhanced_(93-94).csv\n",
      "                                               title  \\\n",
      "0  Diagnosing bias in data-driven algorithms for ...   \n",
      "\n",
      "                                         all_authors  \\\n",
      "0  Jenna Wiens; W. Nicholson Price; Michael W. Sj...   \n",
      "\n",
      "                                    all_affiliations  all_countries abstract  \n",
      "0  University of Michigan; University of Michigan...  USA; USA; USA           \n",
      "Final dataset: 1 publications (including any N/A rows)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "# Split a list into chunks of specified size\n",
    "def chunk_list(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(islice(it, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "# Clean DOI strings by removing common prefixes\n",
    "def clean_doi(doi):\n",
    "    prefixes_to_remove = [\n",
    "        \"https://doi.org/\",\n",
    "        \"http://doi.org/\",\n",
    "        \"doi.org/\",\n",
    "        \"DOI:\",\n",
    "        \"doi:\",\n",
    "        \"https://dx.doi.org/\",\n",
    "        \"http://dx.doi.org/\",\n",
    "    ]\n",
    "\n",
    "    cleaned_doi = doi.strip()\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_doi.startswith(prefix):\n",
    "            cleaned_doi = cleaned_doi[len(prefix) :]\n",
    "            break\n",
    "\n",
    "    return cleaned_doi\n",
    "\n",
    "\n",
    "# Fetch publication metadata from OpenAlex API with retry mechanism\n",
    "def get_publication_data(dois, retries=3, delay=5):\n",
    "    all_results = []\n",
    "    cleaned_dois = [clean_doi(doi) for doi in dois]\n",
    "    print(f\"Cleaned DOIs: {cleaned_dois[:5]}...\")\n",
    "\n",
    "    for chunk in chunk_list(cleaned_dois, 50):\n",
    "        pipe_separated_dois = \"|\".join(chunk)\n",
    "        url = f\"https://api.openalex.org/works?filter=doi:{pipe_separated_dois}&per-page=50\"\n",
    "        print(f\"Fetching -- {url}\")\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                print(f\"Response: {response.status_code}\")\n",
    "                if response.status_code == 200:\n",
    "                    results = response.json().get(\"results\", [])\n",
    "                    print(f\"Found {len(results)} publications in this chunk\")\n",
    "                    all_results.extend(results)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Error {response.status_code} fetching DOIs, retrying... ({attempt+1}/{retries})\"\n",
    "                    )\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request error: {e}, retrying... ({attempt+1}/{retries})\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(f\"Total publications found: {len(all_results)}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Extract grant information from publication data safely\n",
    "def extract_grants(publication):\n",
    "    grants_extracted = []\n",
    "    grants_data = publication.get(\"grants\", [])\n",
    "\n",
    "    if not isinstance(grants_data, list):\n",
    "        return grants_extracted\n",
    "\n",
    "    for grant in grants_data:\n",
    "        if not isinstance(grant, dict):\n",
    "            continue\n",
    "\n",
    "        funder_info = grant.get(\"funder\", {})\n",
    "        if not isinstance(funder_info, dict):\n",
    "            continue\n",
    "\n",
    "        grants_extracted.append(\n",
    "            {\n",
    "                \"funder\": funder_info.get(\"display_name\", None),\n",
    "                \"funder_id\": funder_info.get(\"id\", None),\n",
    "                \"award_id\": grant.get(\"award_id\", None),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return grants_extracted\n",
    "\n",
    "\n",
    "# Extract keywords as a list of display names\n",
    "def extract_keywords(publication):\n",
    "    kw_data = publication.get(\"keywords\", [])\n",
    "    if not isinstance(kw_data, list):\n",
    "        return []\n",
    "\n",
    "    keywords_list = []\n",
    "    for kw in kw_data:\n",
    "        if isinstance(kw, dict):\n",
    "            display_name = kw.get(\"display_name\")\n",
    "            if display_name:\n",
    "                keywords_list.append(display_name)\n",
    "\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "# Extract abstract from publication data\n",
    "def extract_abstract(publication):\n",
    "    \"\"\"Extract abstract from publication data, handling missing abstracts gracefully\"\"\"\n",
    "    abstract_data = publication.get(\"abstract\", None)\n",
    "\n",
    "    if abstract_data is None:\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(abstract_data, str):\n",
    "        return abstract_data.strip()\n",
    "\n",
    "    # Sometimes abstract might be in a different format, handle accordingly\n",
    "    if isinstance(abstract_data, dict):\n",
    "        # Check for common abstract fields\n",
    "        for field in [\"text\", \"content\", \"abstract\", \"summary\"]:\n",
    "            if field in abstract_data and abstract_data[field]:\n",
    "                return str(abstract_data[field]).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Country code to country name mapping\n",
    "COUNTRY_CODE_TO_NAME = {\n",
    "    \"US\": \"USA\",\n",
    "    \"GB\": \"United Kingdom\",\n",
    "    \"CA\": \"Canada\",\n",
    "    # ... (other mappings)\n",
    "    \"NP\": \"Nepal\",\n",
    "}\n",
    "\n",
    "\n",
    "# Extract relevant metadata from publication list\n",
    "def extract_publication_data(publication_data):\n",
    "    publication_rows = []\n",
    "\n",
    "    for publication in publication_data:\n",
    "        authorships = publication.get(\"authorships\", [])\n",
    "        if not isinstance(authorships, list):\n",
    "            continue\n",
    "\n",
    "        keywords_list = extract_keywords(publication)\n",
    "        abstract_text = extract_abstract(publication)\n",
    "\n",
    "        all_authors = []\n",
    "        all_affiliations = []\n",
    "        all_countries = []\n",
    "\n",
    "        for author in authorships:\n",
    "            author_name = author.get(\"author\", {}).get(\"display_name\", None)\n",
    "            if author_name:\n",
    "                all_authors.append(author_name)\n",
    "\n",
    "            author_affiliation = \"\"\n",
    "            author_country = \"\"\n",
    "\n",
    "            institution_info = author.get(\"institutions\")\n",
    "            if (\n",
    "                institution_info\n",
    "                and isinstance(institution_info, list)\n",
    "                and len(institution_info) > 0\n",
    "            ):\n",
    "                primary_inst = institution_info[0]\n",
    "                if isinstance(primary_inst, dict):\n",
    "                    aff = primary_inst.get(\"display_name\")\n",
    "                    if aff:\n",
    "                        author_affiliation = aff\n",
    "\n",
    "                    ctry_code = primary_inst.get(\"country_code\")\n",
    "                    if ctry_code:\n",
    "                        ctry_name = COUNTRY_CODE_TO_NAME.get(ctry_code, ctry_code)\n",
    "                        author_country = ctry_name\n",
    "\n",
    "            all_affiliations.append(author_affiliation)\n",
    "            all_countries.append(author_country)\n",
    "\n",
    "        publication_row = {\n",
    "            \"id\": publication.get(\"id\", None),\n",
    "            \"title\": publication.get(\"title\", None),\n",
    "            \"display_name\": publication.get(\"display_name\", None),\n",
    "            \"all_authors\": \"; \".join(all_authors) if all_authors else \"\",\n",
    "            \"all_affiliations\": \"; \".join(all_affiliations) if all_affiliations else \"\",\n",
    "            \"all_countries\": \"; \".join(all_countries) if all_countries else \"\",\n",
    "            \"doi\": publication.get(\"doi\", None),\n",
    "            \"publication_date\": publication.get(\"publication_date\", None),\n",
    "            \"publication_year\": publication.get(\"publication_year\", None),\n",
    "            \"type\": publication.get(\"type\", None),\n",
    "            \"language\": publication.get(\"language\", None),\n",
    "            \"open_access\": publication.get(\"open_access\", {}).get(\"is_oa\", None),\n",
    "            \"open_access_status\": publication.get(\"open_access\", {}).get(\n",
    "                \"oa_status\", None\n",
    "            ),\n",
    "            \"open_access_url\": publication.get(\"open_access\", {}).get(\"oa_url\", None),\n",
    "            \"cited_by_count\": publication.get(\"cited_by_count\", None),\n",
    "            \"keywords\": \"; \".join(keywords_list) if keywords_list else \"\",\n",
    "            \"grants\": (\n",
    "                \"; \".join(\n",
    "                    [\n",
    "                        grant.get(\"funder\", \"\")\n",
    "                        for grant in extract_grants(publication)\n",
    "                        if grant.get(\"funder\")\n",
    "                    ]\n",
    "                )\n",
    "                if extract_grants(publication)\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"abstract\": abstract_text,  # Added abstract column at the end\n",
    "        }\n",
    "\n",
    "        publication_rows.append(publication_row)\n",
    "\n",
    "    return publication_rows\n",
    "\n",
    "\n",
    "# Order DataFrame according to original DOI list and handle missing entries\n",
    "def order_by_doi_sequence(df, original_dois):\n",
    "    cleaned_original_dois = [clean_doi(doi) for doi in original_dois]\n",
    "\n",
    "    if df.empty:\n",
    "        na_rows = [\n",
    "            {\n",
    "                \"id\": \"N/A\",\n",
    "                \"title\": \"N/A\",\n",
    "                \"display_name\": \"N/A\",\n",
    "                \"all_authors\": \"N/A\",\n",
    "                \"all_affiliations\": \"N/A\",\n",
    "                \"all_countries\": \"N/A\",\n",
    "                \"doi\": f\"https://doi.org/{doi}\",\n",
    "                \"publication_date\": \"N/A\",\n",
    "                \"publication_year\": \"N/A\",\n",
    "                \"type\": \"N/A\",\n",
    "                \"language\": \"N/A\",\n",
    "                \"open_access\": \"N/A\",\n",
    "                \"open_access_status\": \"N/A\",\n",
    "                \"open_access_url\": \"N/A\",\n",
    "                \"cited_by_count\": \"N/A\",\n",
    "                \"keywords\": \"N/A\",\n",
    "                \"grants\": \"N/A\",\n",
    "                \"abstract\": \"N/A\",  # Added abstract field to N/A rows\n",
    "            }\n",
    "            for doi in cleaned_original_dois\n",
    "        ]\n",
    "        return pd.DataFrame(na_rows)\n",
    "\n",
    "    def extract_doi_from_url(doi_url):\n",
    "        if pd.isna(doi_url) or not doi_url:\n",
    "            return \"\"\n",
    "        return (\n",
    "            doi_url.replace(\"https://doi.org/\", \"\")\n",
    "            if doi_url.startswith(\"https://doi.org/\")\n",
    "            else doi_url\n",
    "        )\n",
    "\n",
    "    found_dois = set(extract_doi_from_url(doi) for doi in df[\"doi\"])\n",
    "\n",
    "    missing_dois = [doi for doi in cleaned_original_dois if doi not in found_dois]\n",
    "\n",
    "    na_rows = [\n",
    "        {\n",
    "            \"id\": \"N/A\",\n",
    "            \"title\": \"N/A\",\n",
    "            \"display_name\": \"N/A\",\n",
    "            \"all_authors\": \"N/A\",\n",
    "            \"all_affiliations\": \"N/A\",\n",
    "            \"all_countries\": \"N/A\",\n",
    "            \"doi\": f\"https://doi.org/{doi}\",\n",
    "            \"publication_date\": \"N/A\",\n",
    "            \"publication_year\": \"N/A\",\n",
    "            \"type\": \"N/A\",\n",
    "            \"language\": \"N/A\",\n",
    "            \"open_access\": \"N/A\",\n",
    "            \"open_access_status\": \"N/A\",\n",
    "            \"open_access_url\": \"N/A\",\n",
    "            \"cited_by_count\": \"N/A\",\n",
    "            \"keywords\": \"N/A\",\n",
    "            \"grants\": \"N/A\",\n",
    "            \"abstract\": \"N/A\",  # Added abstract field to missing DOI rows\n",
    "        }\n",
    "        for doi in missing_dois\n",
    "    ]\n",
    "\n",
    "    df_with_na = (\n",
    "        pd.concat([df, pd.DataFrame(na_rows)], ignore_index=True) if na_rows else df\n",
    "    )\n",
    "\n",
    "    doi_to_order = {doi: i for i, doi in enumerate(cleaned_original_dois)}\n",
    "\n",
    "    df_with_na[\"doi_order\"] = df_with_na[\"doi\"].apply(\n",
    "        lambda x: doi_to_order.get(extract_doi_from_url(x), 999999)\n",
    "    )\n",
    "\n",
    "    df_ordered = (\n",
    "        df_with_na.sort_values(\"doi_order\")\n",
    "        .drop(columns=[\"doi_order\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_ordered\n",
    "\n",
    "\n",
    "# Example usage (you'll need to define your DOIs list):\n",
    "# dois = [\"10.1038/nature12373\", \"10.1126/science.1234567\"]  # Add your DOIs here\n",
    "\n",
    "# Fetch publication data\n",
    "print(\"Starting data extraction...\")\n",
    "data = get_publication_data(dois)\n",
    "print(f\"Processing {len(data)} publications...\")\n",
    "\n",
    "if not data:\n",
    "    print(\"No data extracted. Adding N/A rows...\")\n",
    "    df_final = order_by_doi_sequence(pd.DataFrame(), dois)\n",
    "else:\n",
    "    records = extract_publication_data(data)\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    seen_ids = set()\n",
    "    indices_to_keep = [\n",
    "        i\n",
    "        for i, pub_id in enumerate(df[\"id\"])\n",
    "        if not (pub_id in seen_ids or seen_ids.add(pub_id))\n",
    "    ]\n",
    "\n",
    "    df_deduplicated = df.iloc[indices_to_keep].reset_index(drop=True)\n",
    "\n",
    "    df_final = order_by_doi_sequence(df_deduplicated, dois)\n",
    "\n",
    "# Save results to CSV\n",
    "df_final.to_csv(\"abstract_publication_data.csv\", index=False)\n",
    "print(\"Data saved to publication_data_enhanced_(93-94).csv\")\n",
    "print(\n",
    "    df_final[\n",
    "        [\"title\", \"all_authors\", \"all_affiliations\", \"all_countries\", \"abstract\"]\n",
    "    ].head(3)\n",
    ")\n",
    "print(f\"Final dataset: {len(df_final)} publications (including any N/A rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
